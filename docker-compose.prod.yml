# Full production docker compose
# - Includes all core app services (gateway, identity, dashboard, db, storage, postgres, garnet, pgcat)
# - Adds an optional "observability" profile (SigNoz + ClickHouse + OTEL collector)
# - Default posture: internal-only for observability. Only the Gateway publishes a host port (80).
#
# IMPORTANT: Local Testing vs Production Deployment
# ==================================================
# This file supports TWO use cases:
#
# 1. LOCAL TESTING of prod-like compose (HTTP URLs, Development environment):
#    - Uses HTTP for PUBLIC_BASE_URL (e.g., http://127.0.0.1:8080 or http://localhost:8080)
#    - ASPNETCORE_ENVIRONMENT=Development (allows HTTP OIDC metadata)
#    - Purpose: Validate compose file structure and service interaction locally
#
# 2. TRUE PRODUCTION deployment (HTTPS URLs, Production environment):
#    - Uses HTTPS for PUBLIC_BASE_URL (e.g., https://apps.example.com)
#    - ASPNETCORE_ENVIRONMENT=Production (enforces HTTPS OIDC metadata)
#    - TLS termination at gateway or upstream load balancer
#    - Purpose: Secure, production-ready deployment
#
# For local testing with HTTP URLs, services are set to Development environment below.
# For true production with HTTPS, change all ASPNETCORE_ENVIRONMENT to Production.
#
# Usage:
# - Prod-like (local HTTP testing):
#     docker compose -f docker-compose.prod.yml up -d --build
# - Prod-like + Observability (enable profile):
#     docker compose -f docker-compose.prod.yml --profile observability up -d --build
#
# Notes:
# - Replace example domains/secrets below to match your environment.
# - For HTTPS in production, configure TLS at the gateway and uncomment the 443 mapping.
# - Keep SigNoz internal-only; expose the UI only behind VPN/SSO if required.

# Development: Uses :latest for fast iteration
# Production: Pin specific versions to prevent surprise upgrades

services:
  postgres:
    image: ghcr.io/musagursoy/tansucloud-postgres:latest
    container_name: tansu-postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - tansu-pgdata:/var/lib/postgresql/data
      # No bind mount needed - databases are created automatically on first start
      # - ./dev/db-init:/docker-entrypoint-initdb.d:ro
    # In production, keep Postgres internal-only by default. If you need external access for E2E tests or direct admin ops,
    # uncomment the following port mapping and restrict exposure via firewall/security groups.
    # ports:
    #   - "5432:5432"
    networks:
      - tansucloud-network

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: tansu-postgres-exporter
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/postgres?sslmode=disable"
    expose:
      - "9187"
    depends_on:
      postgres:
        condition: service_started
    networks:
      - tansucloud-network
    profiles:
      - observability

  redis:
    image: ghcr.io/microsoft/garnet:latest
    container_name: tansu-redis
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    command: ["/app/GarnetServer", "--port", "6379", "--checkpointdir", "/data", "--recover"]
    healthcheck:
      test: ["CMD", "/app/GarnetServer", "--test-ping"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 5s
    # In production, keep Garnet (Redis-compatible) internal-only by default. If you need external access for tests or ops,
    # uncomment the following port mapping and restrict exposure via firewall/security groups.
    # ports:
    #   - "6379:6379"
    volumes:
      - tansu-garnetdata:/data
    networks:
      - tansucloud-network

  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: tansu-redis-exporter
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    environment:
      REDIS_ADDR: redis:6379
    expose:
      - "9121"
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - tansucloud-network
    profiles:
      - observability

  pgcat:
    image: ghcr.io/postgresml/pgcat:latest
    container_name: tansu-pgcat
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      PGCAT_ADMIN_USER: ${PGCAT_ADMIN_USER}
      PGCAT_ADMIN_PASSWORD: ${PGCAT_ADMIN_PASSWORD}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_HOST: postgres
    volumes:
      - ./dev/pgcat/pgcat.toml:/etc/pgcat/pgcat.toml:ro
    command: ["pgcat", "/etc/pgcat/pgcat.toml"]
    expose:
      - "6432"
      - "9930"
    healthcheck:
      test: ["CMD-SHELL", "cat /proc/1/cmdline | tr '\\0' ' ' | grep -qi pgcat"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 20s
    networks:
      - tansucloud-network

  identity:
    image: ghcr.io/musagursoy/tansucloud-identity:latest
    container_name: tansu-identity
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      # For local HTTP testing, use Development (allows HTTP OIDC). For true production with HTTPS, use Production.
      ASPNETCORE_ENVIRONMENT: Production
      PublicBaseUrl: ${PUBLIC_BASE_URL}
      GatewayBaseUrl: ${GATEWAY_BASE_URL}
      # Production issuer visible to browsers
      Oidc__Issuer: ${PUBLIC_BASE_URL}/identity/
      # OTLP exporter endpoint - defaults to internal collector when observability profile is active
      OpenTelemetry__Otlp__Endpoint: ${OTLP_ENDPOINT:-http://otel-collector:4317}
      # Dashboard client credentials
      Oidc__Dashboard__ClientSecret: ${DASHBOARD_CLIENT_SECRET}
      # Optional: override redirect URIs if not derived automatically
      # Oidc__RedirectUri: ${PUBLIC_BASE_URL}/dashboard/signin-oidc
      # Oidc__PostLogoutRedirectUri: ${PUBLIC_BASE_URL}/dashboard/signout-callback-oidc
      # Oidc__Dashboard__RedirectUri: ${PUBLIC_BASE_URL}/dashboard/signin-oidc
      # Oidc__Dashboard__PostLogoutRedirectUri: ${PUBLIC_BASE_URL}/dashboard/signout-callback-oidc
      ConnectionStrings__Default: "Host=pgcat;Port=6432;Database=tansu_identity;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      Cache__Redis: redis:6379
      # Route audit writes via PgCat
      Audit__ConnectionString: "Host=pgcat;Port=6432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
    healthcheck:
      test: ["CMD", "/bin/busybox", "wget", "-q", "-O", "-", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 25s
    depends_on:
      pgcat:
        condition: service_healthy
      db:
        condition: service_healthy
    networks:
      - tansucloud-network

  dashboard:
    image: ghcr.io/musagursoy/tansucloud-dashboard:latest
    user: "0:0"
    container_name: tansu-dashboard
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      # For local HTTP testing, use Development (allows HTTP OIDC). For true production with HTTPS, use Production.
      ASPNETCORE_ENVIRONMENT: Production
      PublicBaseUrl: ${PUBLIC_BASE_URL}
      Oidc__Authority: ${PUBLIC_BASE_URL}/identity
      # Backchannel discovery uses gateway service name (containers can't reach PUBLIC_BASE_URL from inside Docker)
      Oidc__MetadataAddress: http://gateway:8080/identity/.well-known/openid-configuration
      # For local HTTP testing, set to false. For true production with HTTPS, set to true.
      Oidc__RequireHttpsMetadata: true
      Oidc__ClientId: tansu-dashboard
      Oidc__ClientSecret: ${DASHBOARD_CLIENT_SECRET}
      # OTLP exporter endpoint - defaults to internal collector when observability profile is active
      OpenTelemetry__Otlp__Endpoint: ${OTLP_ENDPOINT:-http://otel-collector:4317}
      Cache__Redis: redis:6379
      # Route audit writes via PgCat; override via env in real deployments
      Audit__ConnectionString: "Host=pgcat;Port=6432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      # Prometheus Query Service API (Task 47 Phase 1 - Prometheus metrics)
      PrometheusQuery__ApiBaseUrl: http://prometheus:9090
      PrometheusQuery__PrometheusUiBaseUrl: http://prometheus:9090
      PrometheusQuery__TimeoutSeconds: 30
      PrometheusQuery__MaxRetries: 2
      # Tempo Query Service API (Task 47 Phase 2 - Tempo traces)
      TempoQuery__ApiBaseUrl: http://tempo:3200
      TempoQuery__TimeoutSeconds: 30
      TempoQuery__MaxRetries: 2
      # Loki Query Service API (Task 47 Phase 3 - Loki logs)
      LokiQuery__ApiBaseUrl: http://loki:3100
      LokiQuery__TimeoutSeconds: 30
      LokiQuery__MaxRetries: 2
      # Grafana Dashboard Embedding (Task 47 Phase 4 - Grafana visualization)
      Grafana__Enabled: ${GRAFANA_ENABLED:-false}
      Grafana__BaseUrl: ${PUBLIC_BASE_URL}/grafana
      # SigNoz Query Service API (inside container network, port 8080)
      SigNozQuery__ApiBaseUrl: http://signoz:8080
      SigNozQuery__Email: ${SIGNOZ_API_EMAIL}
      SigNozQuery__Password: ${SIGNOZ_API_PASSWORD}
      # SigNoz UI accessed directly via Docker network (iframe in Dashboard)
      SigNozQuery__SigNozUiBaseUrl: http://signoz:8080
      GatewayBaseUrl: ${GATEWAY_BASE_URL}
      # Prometheus has been removed in favor of SigNoz. Leave unset to disable legacy metrics proxy.
      # Prometheus__BaseUrl:
      LogReporting__Enabled: ${LOGREPORTING__ENABLED:-true}
      LogReporting__MainServerUrl: ${LOGREPORTING__MAINSERVERURL:-}
      LogReporting__ApiKey: ${LOGREPORTING__APIKEY:-}
      LogReporting__ReportIntervalMinutes: ${LOGREPORTING__REPORTINTERVALMINUTES:-60}
      LogReporting__TenantHashSecret: ${LOGREPORTING__TENANTHASHSECRET:-}
      LogReporting__PseudonymizeTenants: ${LOGREPORTING__PSEUDONYMIZETENANTS:-true}
    volumes:
      - tansu-dashboard-keys:/keys
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "/bin/busybox", "wget", "-q", "-O", "-", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 25s
    networks:
      - tansucloud-network

  db:
    image: ghcr.io/musagursoy/tansucloud-db:latest
    container_name: tansu-db
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      # For local HTTP testing, use Development (allows HTTP OIDC). For true production with HTTPS, use Production.
      ASPNETCORE_ENVIRONMENT: Production
      PublicBaseUrl: ${PUBLIC_BASE_URL}
      GatewayBaseUrl: ${GATEWAY_BASE_URL}
      Oidc__Issuer: ${PUBLIC_BASE_URL}/identity/
      # In production, prefer discovery at the public origin (HTTPS)
      Oidc__MetadataAddress: ${PUBLIC_BASE_URL}/identity/.well-known/openid-configuration
      # OTLP exporter endpoint - defaults to internal collector when observability profile is active
      OpenTelemetry__Otlp__Endpoint: ${OTLP_ENDPOINT:-http://otel-collector:4317}
      Cache__Redis: redis:6379
      Outbox__RedisConnection: redis:6379
      Outbox__Channel: tansu.outbox
      Outbox__PollSeconds: 5
      Outbox__BatchSize: 200
      Outbox__MaxAttempts: 8
      # Optional: specify a tenant for outbox dispatcher context (useful for multi-tenant event routing)
      # Outbox__DispatchTenant: <tenant-id>
      ConnectionStrings__DefaultConnection: "Host=postgres;Port=5432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      Provisioning__AdminConnectionString: "Host=postgres;Port=5432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      # Temporarily use direct PostgreSQL instead of PgCat until dynamic pool management is implemented (Task 46)
      # PgCat HTTP Admin API does not exist - see docs/PgCat-Admin-API-Investigation.md
      Provisioning__RuntimeConnectionString: "Host=postgres;Port=5432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      # Route audit writes directly to PostgreSQL (PgCat pool management pending)
      Audit__ConnectionString: "Host=postgres;Port=5432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      # PgCat Admin API configuration (not currently used - HTTP API does not exist)
      PgCat__AdminBaseUrl: http://pgcat:9930
      PGCAT_ADMIN_USER: ${PGCAT_ADMIN_USER}
      PGCAT_ADMIN_PASSWORD: ${PGCAT_ADMIN_PASSWORD}
    healthcheck:
      test: ["CMD", "/bin/busybox", "wget", "-q", "-O", "-", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 25s
    depends_on:
      pgcat:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - tansucloud-network

  storage:
    image: ghcr.io/musagursoy/tansucloud-storage:latest
    user: "0:0"
    container_name: tansu-storage
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      # For local HTTP testing, use Development (allows HTTP OIDC). For true production with HTTPS, use Production.
      ASPNETCORE_ENVIRONMENT: Production
      PublicBaseUrl: ${PUBLIC_BASE_URL}
      GatewayBaseUrl: ${GATEWAY_BASE_URL}
      Oidc__Issuer: ${PUBLIC_BASE_URL}/identity/
      # In production, prefer discovery at the public origin (HTTPS)
      Oidc__MetadataAddress: ${PUBLIC_BASE_URL}/identity/.well-known/openid-configuration
      # OTLP exporter endpoint - defaults to internal collector when observability profile is active
      OpenTelemetry__Otlp__Endpoint: ${OTLP_ENDPOINT:-http://otel-collector:4317}
      Cache__Redis: redis:6379
      Storage__RootPath: /data
      # Consider providing a strong presign secret via environment/secrets manager in real deployments
      Storage__PresignSecret: ${STORAGE_PRESIGN_SECRET:-change-me}
      Storage__MultipartMaxPartSizeBytes: 1048576
      # Route audit writes via PgCat
      Audit__ConnectionString: "Host=pgcat;Port=6432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
    volumes:
      - tansu-storagedata:/data
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "/bin/busybox", "wget", "-q", "-O", "-", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 25s
    networks:
      - tansucloud-network

  gateway:
    image: ghcr.io/musagursoy/tansucloud-gateway:latest
    container_name: tansu-gateway
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    volumes:
      - tansu-gateway-keys:/keys
    environment:
      # For local HTTP testing, use Development (allows HTTP OIDC). For true production with HTTPS, use Production.
      ASPNETCORE_ENVIRONMENT: Production
      PublicBaseUrl: ${PUBLIC_BASE_URL}
      GatewayBaseUrl: ${GATEWAY_BASE_URL}
      GATEWAY_URLS: http://0.0.0.0:8080
      Kestrel__Endpoints__Http__Url: http://0.0.0.0:8080
      Services__DashboardBaseUrl: http://dashboard:8080
      Services__IdentityBaseUrl: http://identity:8080
      Services__DatabaseBaseUrl: http://db:8080
      Services__StorageBaseUrl: http://storage:8080
      Services__SigNozBaseUrl: http://signoz:8080
      # OTLP exporter endpoint - defaults to internal collector when observability profile is active
      OpenTelemetry__Otlp__Endpoint: ${OTLP_ENDPOINT:-http://otel-collector:4317}
      Cache__Redis: redis:6379
      Gateway__TrustForwardedHeaders: "true"
      # Route audit writes via PgCat
      Audit__ConnectionString: "Host=pgcat;Port=6432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      # Policy Center persistence (shares Identity database)
      ConnectionStrings__GatewayDb: "Host=pgcat;Port=6432;Database=tansu_identity;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
    ports:
      - "80:8080"
      # For E2E test compatibility, also expose on 8080 (matches dev behavior). For true production, comment out this mapping.
      # - "8080:8080"
      # - "443:8443" # enable when TLS is configured for the gateway
    healthcheck:
      test: ["CMD", "/bin/busybox", "wget", "-q", "-O", "-", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 25s
    depends_on:
      postgres:
        condition: service_started
      identity:
        condition: service_healthy
      dashboard:
        condition: service_healthy
      db:
        condition: service_healthy
      storage:
        condition: service_healthy
    networks:
      - tansucloud-network

  # =====================
  # Observability Stack (optional) â€” enable with: --profile observability
  # PGTL: Prometheus (metrics), Tempo (traces), Loki (logs)
  # =====================

  # OpenTelemetry Collector - Routes telemetry to PGTL stack
  otel-collector:
    profiles: [observability]
    image: otel/opentelemetry-collector-contrib:0.114.0
    container_name: tansu-otel-collector
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./dev/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    expose:
      - "4317" # OTLP gRPC
      - "4318" # OTLP HTTP
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:13133/"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    depends_on:
      prometheus:
        condition: service_started
    networks:
      - tansucloud-network

  tempo:
    profiles: [observability]
    image: grafana/tempo:2.6.0
    container_name: tansu-tempo
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
    command:
      - '-config.file=/etc/tempo.yml'
    volumes:
      - ./dev/tempo-config.yml:/etc/tempo.yml:ro
      - tansu-tempo-data:/var/tempo
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3200/ready"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - tansucloud-network

  loki:
    profiles: [observability]
    image: grafana/loki:3.3.1
    container_name: tansu-loki
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
    command:
      - '-config.file=/etc/loki.yml'
    volumes:
      - ./dev/loki-config.yml:/etc/loki.yml:ro
      - tansu-loki-data:/loki
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - tansucloud-network

  prometheus:
    profiles: [observability]
    image: prom/prometheus:v3.0.1
    container_name: tansu-prometheus
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ./dev/prometheus-config.yml:/etc/prometheus/prometheus.yml:ro
      - tansu-prometheus-data:/prometheus
    # NOTE: Prometheus is accessible only within the Docker network via http://prometheus:9090
    # All observability access goes through the Dashboard API - no direct external port exposure
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - tansucloud-network

  grafana:
    profiles: [observability]
    image: grafana/grafana:11.3.1
    container_name: tansu-grafana
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
    environment:
      # Database backend (PostgreSQL)
      GF_DATABASE_TYPE: postgres
      GF_DATABASE_HOST: postgres:5432
      GF_DATABASE_NAME: grafana
      GF_DATABASE_USER: ${POSTGRES_USER}
      GF_DATABASE_PASSWORD: ${POSTGRES_PASSWORD}
      GF_DATABASE_SSL_MODE: disable
      
      # Security
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_AUTH_DISABLE_LOGIN_FORM: "false"
      GF_AUTH_ANONYMOUS_ENABLED: "true"
      GF_AUTH_ANONYMOUS_ORG_ROLE: Viewer
      
      # Server settings (external URL for asset paths; SERVE_FROM_SUB_PATH handles proxy)
      GF_SERVER_ROOT_URL: ${PUBLIC_BASE_URL}/grafana
      GF_SERVER_SERVE_FROM_SUB_PATH: "true"
      GF_SERVER_HTTP_PORT: "3000"
      
      # Security - Allow embedding in iframe (Dashboard embeds Grafana dashboards)
      GF_SECURITY_ALLOW_EMBEDDING: "true"
      
      # Paths
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      GF_PATHS_DATA: /var/lib/grafana
      
      # Logging
      GF_LOG_LEVEL: info
    volumes:
      - tansu-grafana-data:/var/lib/grafana
      - ./dev/grafana/provisioning:/etc/grafana/provisioning:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s
    depends_on:
      postgres:
        condition: service_healthy
      tempo:
        condition: service_healthy
      loki:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    networks:
      - tansucloud-network


networks:
  tansucloud-network:
    driver: bridge

volumes:
  tansu-pgdata:
    driver: local
  tansu-garnetdata:
    driver: local
  tansu-storagedata:
    driver: local
  tansu-gateway-keys:
    driver: local
  tansu-dashboard-keys:
    driver: local
  tansu-prometheus-data:
    driver: local
  tansu-tempo-data:
    driver: local
  tansu-loki-data:
    driver: local
  tansu-grafana-data:
    driver: local
