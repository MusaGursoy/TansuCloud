# Full production docker compose
# - Includes all core app services (gateway, identity, dashboard, db, storage, postgres, garnet, pgcat)
# - Adds an optional "observability" profile (SigNoz + ClickHouse + OTEL collector)
# - Default posture: internal-only for observability. Only the Gateway publishes a host port (80).
#
# IMPORTANT: Local Testing vs Production Deployment
# ==================================================
# This file supports TWO use cases:
#
# 1. LOCAL TESTING of prod-like compose (HTTP URLs, Development environment):
#    - Uses HTTP for PUBLIC_BASE_URL (e.g., http://127.0.0.1:8080 or http://localhost:8080)
#    - ASPNETCORE_ENVIRONMENT=Development (allows HTTP OIDC metadata)
#    - Purpose: Validate compose file structure and service interaction locally
#
# 2. TRUE PRODUCTION deployment (HTTPS URLs, Production environment):
#    - Uses HTTPS for PUBLIC_BASE_URL (e.g., https://apps.example.com)
#    - ASPNETCORE_ENVIRONMENT=Production (enforces HTTPS OIDC metadata)
#    - TLS termination at gateway or upstream load balancer
#    - Purpose: Secure, production-ready deployment
#
# For local testing with HTTP URLs, services are set to Development environment below.
# For true production with HTTPS, change all ASPNETCORE_ENVIRONMENT to Production.
#
# Usage:
# - Prod-like (local HTTP testing):
#     docker compose -f docker-compose.prod.yml up -d --build
# - Prod-like + Observability (enable profile):
#     docker compose -f docker-compose.prod.yml --profile observability up -d --build
#
# Notes:
# - Replace example domains/secrets below to match your environment.
# - For HTTPS in production, configure TLS at the gateway and uncomment the 443 mapping.
# - Keep SigNoz internal-only; expose the UI only behind VPN/SSO if required.

# Development: Uses :latest for fast iteration
# Production: Pin specific versions to prevent surprise upgrades

services:
  postgres:
    image: tansu/citus-pgvector:local
    container_name: tansu-postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - tansu-pgdata:/var/lib/postgresql/data
      # Init scripts (01-init.sql) are baked into the Docker image (see dev/Dockerfile.citus-pgvector)
      # No bind mount needed - databases are created automatically on first start
      # - ./dev/db-init:/docker-entrypoint-initdb.d:ro
    # In production, keep Postgres internal-only by default. If you need external access for E2E tests or direct admin ops,
    # uncomment the following port mapping and restrict exposure via firewall/security groups.
    # ports:
    #   - "5432:5432"
    networks:
      - tansucloud-network

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: tansu-postgres-exporter
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/postgres?sslmode=disable"
    expose:
      - "9187"
    depends_on:
      postgres:
        condition: service_started
    networks:
      - tansucloud-network
    profiles:
      - observability

  redis:
    image: ghcr.io/microsoft/garnet:latest
    container_name: tansu-redis
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    command: ["/app/GarnetServer", "--port", "6379", "--checkpointdir", "/data", "--recover"]
    healthcheck:
      test: ["CMD", "/app/GarnetServer", "--test-ping"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 5s
    # In production, keep Garnet (Redis-compatible) internal-only by default. If you need external access for tests or ops,
    # uncomment the following port mapping and restrict exposure via firewall/security groups.
    # ports:
    #   - "6379:6379"
    volumes:
      - tansu-garnetdata:/data
    networks:
      - tansucloud-network

  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: tansu-redis-exporter
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    environment:
      REDIS_ADDR: redis:6379
    expose:
      - "9121"
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - tansucloud-network
    profiles:
      - observability

  pgcat:
    image: ghcr.io/postgresml/pgcat:latest
    container_name: tansu-pgcat
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      PGCAT_ADMIN_USER: ${PGCAT_ADMIN_USER}
      PGCAT_ADMIN_PASSWORD: ${PGCAT_ADMIN_PASSWORD}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_HOST: postgres
    volumes:
      - ./dev/pgcat/pgcat.toml:/etc/pgcat/pgcat.toml:ro
    command: ["pgcat", "/etc/pgcat/pgcat.toml"]
    expose:
      - "6432"
      - "9930"
    healthcheck:
      test: ["CMD-SHELL", "cat /proc/1/cmdline | tr '\\0' ' ' | grep -qi pgcat"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 20s
    networks:
      - tansucloud-network

  identity:
    build:
      context: .
      dockerfile: TansuCloud.Identity/Dockerfile
    container_name: tansu-identity
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      # For local HTTP testing, use Development (allows HTTP OIDC). For true production with HTTPS, use Production.
      ASPNETCORE_ENVIRONMENT: Production
      PublicBaseUrl: ${PUBLIC_BASE_URL}
      GatewayBaseUrl: ${GATEWAY_BASE_URL}
      # Production issuer visible to browsers
      Oidc__Issuer: ${PUBLIC_BASE_URL}/identity/
      # OTLP exporter endpoint - defaults to internal collector when observability profile is active
      OpenTelemetry__Otlp__Endpoint: ${OTLP_ENDPOINT:-http://signoz-otel-collector:4317}
      # Dashboard client credentials
      Oidc__Dashboard__ClientSecret: ${DASHBOARD_CLIENT_SECRET}
      # Optional: override redirect URIs if not derived automatically
      # Oidc__RedirectUri: ${PUBLIC_BASE_URL}/dashboard/signin-oidc
      # Oidc__PostLogoutRedirectUri: ${PUBLIC_BASE_URL}/dashboard/signout-callback-oidc
      # Oidc__Dashboard__RedirectUri: ${PUBLIC_BASE_URL}/dashboard/signin-oidc
      # Oidc__Dashboard__PostLogoutRedirectUri: ${PUBLIC_BASE_URL}/dashboard/signout-callback-oidc
      ConnectionStrings__Default: "Host=pgcat;Port=6432;Database=tansu_identity;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      Cache__Redis: redis:6379
      # Route audit writes via PgCat
      Audit__ConnectionString: "Host=pgcat;Port=6432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
    healthcheck:
      test: ["CMD", "/bin/busybox", "wget", "-q", "-O", "-", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 25s
    depends_on:
      pgcat:
        condition: service_healthy
    networks:
      - tansucloud-network

  dashboard:
    build:
      context: .
      dockerfile: TansuCloud.Dashboard/Dockerfile
    user: "0:0"
    container_name: tansu-dashboard
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      # For local HTTP testing, use Development (allows HTTP OIDC). For true production with HTTPS, use Production.
      ASPNETCORE_ENVIRONMENT: Production
      PublicBaseUrl: ${PUBLIC_BASE_URL}
      Oidc__Authority: ${PUBLIC_BASE_URL}/identity
      # Backchannel discovery uses gateway service name (containers can't reach PUBLIC_BASE_URL from inside Docker)
      Oidc__MetadataAddress: http://gateway:8080/identity/.well-known/openid-configuration
      # For local HTTP testing, set to false. For true production with HTTPS, set to true.
      Oidc__RequireHttpsMetadata: true
      Oidc__ClientId: tansu-dashboard
      Oidc__ClientSecret: ${DASHBOARD_CLIENT_SECRET}
      # OTLP exporter endpoint - defaults to internal collector when observability profile is active
      OpenTelemetry__Otlp__Endpoint: ${OTLP_ENDPOINT:-http://signoz-otel-collector:4317}
      Cache__Redis: redis:6379
      # Route audit writes via PgCat; override via env in real deployments
      Audit__ConnectionString: "Host=pgcat;Port=6432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      GatewayBaseUrl: ${GATEWAY_BASE_URL}
      # Prometheus has been removed in favor of SigNoz. Leave unset to disable legacy metrics proxy.
      # Prometheus__BaseUrl:
      LogReporting__Enabled: ${LOGREPORTING__ENABLED:-true}
      LogReporting__MainServerUrl: ${LOGREPORTING__MAINSERVERURL:-}
      LogReporting__ApiKey: ${LOGREPORTING__APIKEY:-}
      LogReporting__ReportIntervalMinutes: ${LOGREPORTING__REPORTINTERVALMINUTES:-60}
      LogReporting__TenantHashSecret: ${LOGREPORTING__TENANTHASHSECRET:-}
      LogReporting__PseudonymizeTenants: ${LOGREPORTING__PSEUDONYMIZETENANTS:-true}
    volumes:
      - tansu-dashboard-keys:/keys
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "/bin/busybox", "wget", "-q", "-O", "-", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 25s
    networks:
      - tansucloud-network

  db:
    build:
      context: .
      dockerfile: TansuCloud.Database/Dockerfile
    container_name: tansu-db
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      # For local HTTP testing, use Development (allows HTTP OIDC). For true production with HTTPS, use Production.
      ASPNETCORE_ENVIRONMENT: Production
      PublicBaseUrl: ${PUBLIC_BASE_URL}
      GatewayBaseUrl: ${GATEWAY_BASE_URL}
      Oidc__Issuer: ${PUBLIC_BASE_URL}/identity/
      # In production, prefer discovery at the public origin (HTTPS)
      Oidc__MetadataAddress: ${PUBLIC_BASE_URL}/identity/.well-known/openid-configuration
      # OTLP exporter endpoint - defaults to internal collector when observability profile is active
      OpenTelemetry__Otlp__Endpoint: ${OTLP_ENDPOINT:-http://signoz-otel-collector:4317}
      Cache__Redis: redis:6379
      Outbox__RedisConnection: redis:6379
      Outbox__Channel: tansu.outbox
      Outbox__PollSeconds: 5
      Outbox__BatchSize: 200
      Outbox__MaxAttempts: 8
      # Optional: specify a tenant for outbox dispatcher context (useful for multi-tenant event routing)
      # Outbox__DispatchTenant: <tenant-id>
      ConnectionStrings__DefaultConnection: "Host=postgres;Port=5432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      Provisioning__AdminConnectionString: "Host=postgres;Port=5432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      Provisioning__RuntimeConnectionString: "Host=pgcat;Port=6432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      # Route audit writes via PgCat
      Audit__ConnectionString: "Host=pgcat;Port=6432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      # PgCat Admin API for dynamic pool management (zero-downtime tenant provisioning)
      PgCat__AdminBaseUrl: http://pgcat:9930
      PGCAT_ADMIN_USER: ${PGCAT_ADMIN_USER}
      PGCAT_ADMIN_PASSWORD: ${PGCAT_ADMIN_PASSWORD}
    healthcheck:
      test: ["CMD", "/bin/busybox", "wget", "-q", "-O", "-", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 25s
    depends_on:
      pgcat:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - tansucloud-network

  storage:
    build:
      context: .
      dockerfile: TansuCloud.Storage/Dockerfile
    user: "0:0"
    container_name: tansu-storage
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      # For local HTTP testing, use Development (allows HTTP OIDC). For true production with HTTPS, use Production.
      ASPNETCORE_ENVIRONMENT: Production
      PublicBaseUrl: ${PUBLIC_BASE_URL}
      GatewayBaseUrl: ${GATEWAY_BASE_URL}
      Oidc__Issuer: ${PUBLIC_BASE_URL}/identity/
      # In production, prefer discovery at the public origin (HTTPS)
      Oidc__MetadataAddress: ${PUBLIC_BASE_URL}/identity/.well-known/openid-configuration
      # OTLP exporter endpoint - defaults to internal collector when observability profile is active
      OpenTelemetry__Otlp__Endpoint: ${OTLP_ENDPOINT:-http://signoz-otel-collector:4317}
      Cache__Redis: redis:6379
      Storage__RootPath: /data
      # Consider providing a strong presign secret via environment/secrets manager in real deployments
      Storage__PresignSecret: ${STORAGE_PRESIGN_SECRET:-change-me}
      Storage__MultipartMaxPartSizeBytes: 1048576
      # Route audit writes via PgCat
      Audit__ConnectionString: "Host=pgcat;Port=6432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
    volumes:
      - tansu-storagedata:/data
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "/bin/busybox", "wget", "-q", "-O", "-", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 25s
    networks:
      - tansucloud-network

  gateway:
    build:
      context: .
      dockerfile: TansuCloud.Gateway/Dockerfile
    container_name: tansu-gateway
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    volumes:
      - tansu-gateway-keys:/keys
    environment:
      # For local HTTP testing, use Development (allows HTTP OIDC). For true production with HTTPS, use Production.
      ASPNETCORE_ENVIRONMENT: Production
      PublicBaseUrl: ${PUBLIC_BASE_URL}
      GatewayBaseUrl: ${GATEWAY_BASE_URL}
      GATEWAY_URLS: http://0.0.0.0:8080
      Kestrel__Endpoints__Http__Url: http://0.0.0.0:8080
      Services__DashboardBaseUrl: http://dashboard:8080
      Services__IdentityBaseUrl: http://identity:8080
      Services__DatabaseBaseUrl: http://db:8080
      Services__StorageBaseUrl: http://storage:8080
      Services__SigNozBaseUrl: http://signoz-frontend:3301
      # OTLP exporter endpoint - defaults to internal collector when observability profile is active
      OpenTelemetry__Otlp__Endpoint: ${OTLP_ENDPOINT:-http://signoz-otel-collector:4317}
      Cache__Redis: redis:6379
      Gateway__TrustForwardedHeaders: "true"
      # Route audit writes via PgCat
      Audit__ConnectionString: "Host=pgcat;Port=6432;Database=postgres;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
      # Policy Center persistence (shares Identity database)
      ConnectionStrings__GatewayDb: "Host=pgcat;Port=6432;Database=tansu_identity;Username=${POSTGRES_USER};Password=${POSTGRES_PASSWORD}"
    ports:
      - "80:8080"
      # For E2E test compatibility, also expose on 8080 (matches dev behavior). For true production, comment out this mapping.
      # - "8080:8080"
      # - "443:8443" # enable when TLS is configured for the gateway
    healthcheck:
      test: ["CMD", "/bin/busybox", "wget", "-q", "-O", "-", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 25s
    depends_on:
      postgres:
        condition: service_started
      identity:
        condition: service_healthy
      dashboard:
        condition: service_healthy
      db:
        condition: service_healthy
      storage:
        condition: service_healthy
    networks:
      - tansucloud-network

  # =====================
  # Observability (optional) â€” enable with: --profile observability
  # =====================

  zookeeper:
    profiles: [observability]
    image: bitnami/zookeeper:latest
    container_name: signoz-zookeeper
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      - ZOO_SERVER_ID=1
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOO_AUTOPURGE_INTERVAL=1
      - ZOO_ENABLE_PROMETHEUS_METRICS=yes
      - ZOO_4LW_COMMANDS_WHITELIST=ruok
    healthcheck:
      test: ["CMD", "/bin/bash", "-lc", "wget -q -O - http://127.0.0.1:8080/commands/ruok | grep imok"]
      interval: 30s
      timeout: 5s
      retries: 5
    volumes:
      - signoz-zookeeper-data:/bitnami/zookeeper
    networks:
      - tansucloud-network

  clickhouse:
    profiles: [observability]
    image: clickhouse/clickhouse-server:latest
    container_name: signoz-clickhouse
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      CLICKHOUSE_DB: signoz
      CLICKHOUSE_USER: admin
      CLICKHOUSE_PASSWORD: admin
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "0.0.0.0:8123/ping"]
      interval: 30s
      timeout: 5s
      retries: 5
    volumes:
      - signoz-clickhouse-data:/var/lib/clickhouse
      - ./dev/clickhouse/cluster.xml:/etc/clickhouse-server/config.d/cluster.xml:ro
      - ./dev/clickhouse/zookeeper.xml:/etc/clickhouse-server/config.d/zookeeper.xml:ro
    networks:
      - tansucloud-network
    depends_on:
      - zookeeper

  signoz:
    profiles: [observability]
    image: signoz/signoz:latest
    container_name: signoz
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      - SIGNOZ_TELEMETRYSTORE_PROVIDER=clickhouse
      - SIGNOZ_TELEMETRYSTORE_CLICKHOUSE_DSN=tcp://admin:admin@clickhouse:9000
      - SIGNOZ_TELEMETRYSTORE_CLICKHOUSE_CLUSTER=cluster
      - SIGNOZ_ALERTMANAGER_PROVIDER=signoz
      - SIGNOZ_JWT_SECRET=${SIGNOZ_JWT_SECRET:-change-me}
    depends_on:
      clickhouse:
        condition: service_healthy
    volumes:
      - signoz-data:/var/lib/signoz
    networks:
      - tansucloud-network

  signoz-otel-collector:
    profiles: [observability]
    image: signoz/signoz-otel-collector:latest
    container_name: signoz-otel-collector
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    command:
      - --config=/etc/otel-collector-config.yaml
    environment:
      - OTEL_RESOURCE_ATTRIBUTES=host.name=signoz-host,os.type=linux
      - LOW_CARDINAL_EXCEPTION_GROUPING=false
    expose:
      - "4317" # OTLP gRPC
      - "4318" # OTLP HTTP
    volumes:
      - ./dev/signoz-otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    depends_on:
      - signoz
      - signoz-schema-migrator-sync
    networks:
      - tansucloud-network

  clickhouse-prepatch:
    profiles: [observability]
    image: clickhouse/clickhouse-server:latest
    container_name: signoz-clickhouse-prepatch
    restart: on-failure
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
    depends_on:
      clickhouse:
        condition: service_healthy
    networks:
      - tansucloud-network
    entrypoint: ["bash","-lc"]
    volumes:
      - ./dev/clickhouse/prepatches:/prepatches:ro
    command:
      - >-
        bash /prepatches/run-prepatches.sh

  clickhouse-compat-init:
    profiles: [observability]
    image: clickhouse/clickhouse-client:latest
    container_name: signoz-clickhouse-compat-init
    restart: on-failure
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    depends_on:
      clickhouse:
        condition: service_healthy
      clickhouse-prepatch:
        condition: service_completed_successfully
    networks:
      - tansucloud-network
    entrypoint: ["bash","-c"]
    volumes:
      - ./dev/clickhouse/patches:/patches:ro
    command: |
      set -euo pipefail
      clickhouse-client --host clickhouse --port 9000 --user admin --password admin -q "CREATE MATERIALIZED VIEW IF NOT EXISTS signoz_traces.root_operations ON CLUSTER cluster AS SELECT DISTINCT name, resource_string_service\$\$name AS serviceName FROM signoz_traces.signoz_index_v3 WHERE parent_span_id = ''"
      clickhouse-client --host clickhouse --port 9000 --user admin --password admin -q "CREATE MATERIALIZED VIEW IF NOT EXISTS signoz_traces.distributed_top_level_operations ON CLUSTER cluster AS SELECT name, serviceName, toDateTime(timestamp) AS time FROM signoz_traces.distributed_signoz_index_v3 WHERE parent_span_id = '' OR length(parent_span_id) = 0"
      for f in /patches/*.sql; do echo "Applying patch: $f"; clickhouse-client --host clickhouse --port 9000 --user admin --password admin -n < "$f"; done

  signoz-schema-migrator-sync:
    profiles: [observability]
    image: signoz/signoz-schema-migrator:latest
    container_name: signoz-schema-migrator-sync
    command: ["sync", "--dsn=tcp://admin:admin@clickhouse:9000"]
    restart: on-failure
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      - SIGNOZ_TELEMETRYSTORE_CLICKHOUSE_CLUSTER=cluster
    depends_on:
      clickhouse:
        condition: service_healthy
      clickhouse-prepatch:
        condition: service_completed_successfully
      clickhouse-compat-init:
        condition: service_completed_successfully
    networks:
      - tansucloud-network

  signoz-schema-migrator-async:
    profiles: [observability]
    image: signoz/signoz-schema-migrator:latest
    container_name: signoz-schema-migrator-async
    command: ["async", "--dsn=tcp://admin:admin@clickhouse:9000"]
    restart: on-failure
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
    environment:
      - SIGNOZ_TELEMETRYSTORE_CLICKHOUSE_CLUSTER=cluster
    depends_on:
      clickhouse:
        condition: service_healthy
      signoz-schema-migrator-sync:
        condition: service_completed_successfully
    networks:
      - tansucloud-network


networks:
  tansucloud-network:
    driver: bridge

volumes:
  tansu-pgdata:
    driver: local
  tansu-garnetdata:
    driver: local
  tansu-storagedata:
    driver: local
  tansu-gateway-keys:
    driver: local
  tansu-dashboard-keys:
    driver: local
  signoz-clickhouse-data:
    driver: local
  signoz-zookeeper-data:
    driver: local
  signoz-data:
    driver: local
